{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from pyro import poutine\n",
    "from pyro import optim\n",
    "from pyro import infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'simple_english_wikipedia/'\n",
    "CORPUS = 'corpus.txt'\n",
    "KEEP_WORDS = 100\n",
    "\n",
    "def load_sentences_from_raw(keep_words=KEEP_WORDS):\n",
    "    # load the text, remove extra characters, and split into sentences\n",
    "    with open(os.path.join(DATA, CORPUS)) as fh:\n",
    "        # load and make lower\n",
    "        text = fh.read().lower()\n",
    "\n",
    "    # remove non-word, space, or period characters\n",
    "    text = re.sub(r'[^\\s\\dA-Za-z.]', '', text)\n",
    "    # get rid of headings\n",
    "    text = re.sub(r'\\n\\S+\\n', ' ', '\\n' + text)\n",
    "    # change all whitespace to single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # replace numbers with \"NUM\"\n",
    "    text = re.sub(r'\\d+', 'NUM', text)\n",
    "    # compile the most common words\n",
    "    # split on periods\n",
    "    sentences = text.split('. ')\n",
    "    # identify the most common words\n",
    "    counter = Counter(text.replace('.', '').split())\n",
    "    most_common = {w[0]: torch.tensor(i+1) \\\n",
    "                   for i, w in enumerate(counter.most_common(keep_words))}\n",
    "    with open(os.path.join(DATA, 'hash_dict.pkl'), 'wb') as fh:\n",
    "        pickle.dump(most_common, fh)\n",
    "    # create hashed versions of the sentences\n",
    "    def hash_sentence(sentence):\n",
    "        iterator =  (most_common.get(w) for w in sentence.split())\n",
    "        # encode uncommon words as zeros\n",
    "        return torch.stack([x if x is not None else torch.tensor(0) \\\n",
    "                            for x in iterator]).to(torch.int64)\n",
    "    hashed_sentences = [hash_sentence(sentence) for sentence in sentences if sentence]\n",
    "    shuffle(hashed_sentences)\n",
    "    return hashed_sentences\n",
    "\n",
    "def load_sentences(group, use_prehash=True, keep_words=100, n_groups=50):\n",
    "    if use_prehash == False or not os.path.isfile(os.path.join(DATA, f'prehash{group}.pkl')):\n",
    "        hashed_sentences = load_sentences_from_raw(keep_words)\n",
    "        total_size = len(hashed_sentences)\n",
    "        step = total_size // (n_groups - (0 if total_size % n_groups == 0 else 1))\n",
    "        for g, i, in enumerate(range(0, total_size, step)):\n",
    "            with open(os.path.join(DATA, f'prehash{g}.pkl'), 'wb') as fh:\n",
    "                pickle.dump(hashed_sentences[i:(i+step)], fh)\n",
    "        return hashed_sentences[step*group:step*(group+1)]\n",
    "    else:\n",
    "        with open(os.path.join(DATA, f'prehash{group}.pkl'), 'rb') as fh:\n",
    "            hashed_sentences = pickle.load(fh)\n",
    "    return hashed_sentences\n",
    "\n",
    "def get_group(group):\n",
    "    hashed_sentences = load_sentences(group)\n",
    "    sentence_lengths = torch.from_numpy(np.array([len(sentence) for sentence in hashed_sentences]))\n",
    "    # change hashed_sentences to padded tensors\n",
    "    hashed_sentences = nn.utils.rnn.pad_sequence(hashed_sentences, batch_first=True)\n",
    "    # make one-hot so it's compatible with rnn\n",
    "    hashed_sentences = nn.functional.one_hot(hashed_sentences)\n",
    "    return hashed_sentences, sentence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(batch, seq_lengths):\n",
    "    mask = torch.zeros(batch.shape[0:2])\n",
    "    for i in range(batch.shape[0]):\n",
    "        mask[i, 0:seq_lengths[i]] = torch.ones(seq_lengths[i])\n",
    "    return mask\n",
    "    \n",
    "def reverse_seqs(batch, seq_lengths):\n",
    "    \"\"\"Utility function for reversing rnn and mini batch vectors\n",
    "    \"\"\"\n",
    "    reversed_seq = torch.zeros_like(batch)\n",
    "    for i in range(batch.size(0)):\n",
    "        T = seq_lengths[i]\n",
    "        time_slice = torch.arange(T-1, -1, -1)\n",
    "        reversed_seq[i, 0:T, :] = torch.index_select(batch[i, :, :], 0, time_slice)\n",
    "    return reversed_seq\n",
    "\n",
    "def prep(x, x_lengths):\n",
    "    \"\"\"Given batch x and lengths x_lengths, sorts and creates mask and reversed x\"\"\"\n",
    "    # sort from longest to shortest\n",
    "    sorted_length_indices = torch.argsort(x_lengths, descending=True)\n",
    "    x_lengths = x_lengths[sorted_length_indices]\n",
    "    # cut off the unnecessary padding\n",
    "    max_length = torch.max(x_lengths)\n",
    "    x = x[sorted_length_indices, 0:max_length, :]\n",
    "    # reverse and pack to prepare for input to rnn\n",
    "    x_reversed = nn.utils.rnn.pack_padded_sequence(\n",
    "        reverse_seqs(x, x_lengths),\n",
    "        x_lengths,\n",
    "        batch_first=True\n",
    "    )\n",
    "    x_mask = make_mask(x, x_lengths)\n",
    "    return x, x_reversed, x_mask, x_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emitter(nn.Module):\n",
    "    \"\"\"Produces an output vector p(xt | zt), the probability params of a given xt given\n",
    "        the latent variables zt at node t\n",
    "    xt is a vector of 1s and 0s, so this essentially generates a vectorized Bernoulli likelihood\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, x_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # initialize layers for network with 2 hidden layers\n",
    "        self.lin1 = nn.Linear(z_dim, hidden_dim)\n",
    "        self.lin2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin3 = nn.Linear(hidden_dim, x_dim)\n",
    "        # init nonlinearities\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, zt):\n",
    "        h1 = self.relu(self.lin1(zt))\n",
    "        h2 = self.relu(self.lin2(h1))\n",
    "        probs = self.sigmoid(self.lin3(h2))\n",
    "        return probs\n",
    "\n",
    "        \n",
    "class Transition(nn.Module):\n",
    "    \"\"\"Given zt_1, produces loc and scale parameters for the distribution of zt\n",
    "    Making this a gated transition might improve fit?\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # init layers for loc and scale, one hidden for each\n",
    "        self.z_to_hidden_loc = nn.Linear(z_dim, hidden_dim)\n",
    "        self.hidden_to_loc = nn.Linear(hidden_dim, z_dim)\n",
    "        self.z_to_hidden_scale = nn.Linear(z_dim, hidden_dim)\n",
    "        self.hidden_to_scale = nn.Linear(hidden_dim, z_dim)\n",
    "        # init nonlinearities\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, zt_1):\n",
    "        loc = self.hidden_to_loc(\n",
    "            self.relu(self.z_to_hidden_loc(zt_1))\n",
    "        )\n",
    "        scale = self.softplus(self.hidden_to_scale(\n",
    "            self.relu(self.z_to_hidden_scale(zt_1))\n",
    "        ))\n",
    "        return loc, scale\n",
    "\n",
    "    \n",
    "class GuideNet(nn.Module):\n",
    "    \"\"\"Combines RNN output and zt_1 to produce parameters for variational distribution\n",
    "    variational dist. has form q(zt | zt_1, x{t:T})\n",
    "    The job here is analagous to what Transition does, but for the variational dist.\n",
    "    \"\"\"\n",
    "    def __init__(self, z_dim, rnn_dim):\n",
    "        super().__init__()\n",
    "        # init linear transformations, 1 hidden layer for loc and scale\n",
    "        self.z_to_hidden = nn.Linear(z_dim, rnn_dim)\n",
    "        self.hidden_to_loc = nn.Linear(rnn_dim, z_dim)\n",
    "        self.hidden_to_scale = nn.Linear(rnn_dim, z_dim)\n",
    "        # init nonlinearities\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "    \n",
    "    def forward(self, zt_1, rnn_out):\n",
    "        # take mean of first layer and rnn_out\n",
    "        combined = 0.5 * (self.relu(self.z_to_hidden(zt_1)) + rnn_out)\n",
    "        loc = self.hidden_to_loc(combined)\n",
    "        scale = self.softplus(self.hidden_to_scale(combined))\n",
    "        return loc, scale\n",
    "\n",
    "   \n",
    "class DeepMarkov(nn.Module):\n",
    "    \n",
    "    def __init__(self, x_dim=KEEP_WORDS+1, z_dim=50, transition_dim=100, emitter_dim=50,\n",
    "                 rnn_dim=200, rnn_dropout=0.0):\n",
    "        super().__init__()\n",
    "        # init neural nets used in model and guide\n",
    "        self.emitter = Emitter(z_dim, x_dim, emitter_dim)\n",
    "        self.transition = Transition(z_dim, transition_dim)\n",
    "        self.rnn = nn.RNN(x_dim, hidden_size=rnn_dim, nonlinearity='relu',\n",
    "                          batch_first=True, dropout=rnn_dropout)\n",
    "        self.guideNet = GuideNet(z_dim, rnn_dim)\n",
    "        # define trainable parameters for the z's on the first node\n",
    "        # z0 is for the model; zq0 is for the guide\n",
    "        self.z0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.zq0 = nn.Parameter(torch.zeros(z_dim))\n",
    "        self.svi = None\n",
    "    \n",
    "    def model(self, x, x_reversed, x_mask, x_lengths, annealing_factor=1.0):\n",
    "        T = x.size(1)\n",
    "        # register self with Pyro\n",
    "        pyro.module('deep_markov', self)\n",
    "        # z_prev is z on the previous node; expand to have same length as x\n",
    "        z_prev = self.z0.expand(x.size(0), self.z0.size(0))\n",
    "        with pyro.plate('z_batch', len(x)):\n",
    "            for t in range(1, T+1):\n",
    "                # sample zt ~ p(zt | zt_1)\n",
    "                z_loc, z_scale = self.transition(z_prev)  # get params\n",
    "                # scale log probabilities for KL annealing\n",
    "                with poutine.scale(None, annealing_factor):\n",
    "                    # mask to deal with uneven lengths on x vector\n",
    "                    zt = pyro.sample(f'z{t}', dist.Normal(z_loc, z_scale)\n",
    "                                     .mask(x_mask[:, (t-1):t]).to_event(1))\n",
    "                # compute likelihood p(xt | zt)\n",
    "                emitter_probs = self.emitter(zt)\n",
    "                pyro.sample(f'obs_x{t}', dist.OneHotCategorical(probs=emitter_probs)\n",
    "                            .mask(x_mask[:, (t-1):t]).to_event(1),\n",
    "                            obs=x[:, t-1, :])\n",
    "                # set new z_prev as this zt\n",
    "                z_prev = zt\n",
    "    \n",
    "    def guide(self, x, x_reversed, x_mask, x_lengths, annealing_factor=1.0):\n",
    "        T = x.size(1)\n",
    "        pyro.module('deep_markov', self)\n",
    "        rnn_out, _ = self.rnn(x_reversed.float())\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out, batch_first=True)\n",
    "        rnn_out = reverse_seqs(rnn_out, x_lengths)\n",
    "        z_prev = self.z0.expand(x.size(0), self.z0.size(0))\n",
    "        with pyro.plate('z_batch', len(x)):\n",
    "            for t in range(1, T-1):\n",
    "                # sample from q(zt | zt_1, x{t:T})\n",
    "                z_loc, z_scale = self.guideNet(z_prev, rnn_out[:, (t-1), :])\n",
    "                with pyro.poutine.scale(None, annealing_factor):\n",
    "                    zt = pyro.sample(f'z{t}', dist.Normal(z_loc, z_scale)\n",
    "                                     .mask(x_mask[:, (t-1):t]).to_event(1))\n",
    "                z_prev = zt\n",
    "    \n",
    "    def fit(self, x_train, x_train_lengths,\n",
    "            lr=0.001, clip_norm=10.0, batch_size=32, n_epochs=1,\n",
    "            min_anneal=0.2, anneal_step_per_batch=0.001):\n",
    "        if self.svi is None:\n",
    "            optimizer = optim.ClippedAdam({'lr': lr, 'clip_norm': clip_norm})\n",
    "            self.svi = infer.SVI(self.model, self.guide, optimizer, infer.Trace_ELBO())\n",
    "        total_size = len(x_train)\n",
    "        annealing_factor = min_anneal\n",
    "        print(f'batches per epoch: {total_size // batch_size + 1}')\n",
    "        losses = []\n",
    "        for epoch in range(n_epochs):\n",
    "            epoch_loss = 0\n",
    "            time0 = time()\n",
    "            for i in range(0, total_size, batch_size):\n",
    "                x = x_train[i:(i+batch_size)]\n",
    "                x_lengths = x_train_lengths[i:(i+batch_size)]\n",
    "                x, x_reversed, x_mask, x_lengths = prep(x, x_lengths)\n",
    "                annealing_factor = min(1.0, annealing_factor + anneal_step_per_batch)\n",
    "                epoch_loss += self.svi.step(x, x_reversed, x_mask, x_lengths, annealing_factor)\n",
    "            losses.append(epoch_loss / total_size)\n",
    "            print(f'Epoch {epoch+1}: loss = {epoch_loss / total_size:.5f}, time = {time() - time0:.2f}')\n",
    "        return losses\n",
    "    \n",
    "    def evaluate(self, x_test, x_test_lengths, batch_size=32):\n",
    "        assert self.svi is not None, 'Must run fit first'\n",
    "        total_size = len(x_test)\n",
    "        loss = 0\n",
    "        for i in range(0, total_size, batch_size):\n",
    "            x = x_test[i:(i+batch_size)]\n",
    "            x_lengths = x_test_lengths[i:(i+batch_size)]\n",
    "            x, x_reversed, x_mask, x_lengths = prep(x, x_lengths)\n",
    "            loss += self.svi.evaluate_loss(x, x_reversed, x_mask, x_lengths)\n",
    "        print(f'test loss is {loss / total_size}')\n",
    "        return loss / total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches per epoch: 233\n",
      "Epoch 1: loss = 1119.55199, time = 90.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mckay/anaconda3/lib/python3.7/site-packages/pyro/infer/trace_elbo.py:138: UserWarning: Encountered NaN: loss\n",
      "  warn_if_nan(loss, \"loss\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss = nan, time = 93.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mckay/anaconda3/lib/python3.7/site-packages/pyro/infer/trace_elbo.py:71: UserWarning: Encountered NaN: loss\n",
      "  warn_if_nan(loss, \"loss\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss is {loss / total_size}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_train_lengths = get_group(0)\n",
    "x_test, x_test_lengths = get_group(1)\n",
    "\n",
    "pyro.clear_param_store()\n",
    "deepMarkov = DeepMarkov()\n",
    "deepMarkov.fit(x_train, x_train_lengths, n_epochs=2)\n",
    "deepMarkov.evaluate(x_test, x_test_lengths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
